{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1da9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "CONFIG = {\n",
    "    \"resume_training\": None,  # { \"run_id\": None,\"version\": 0},  #e.g. if 'resume_training' or 'run_id' is None => no resume\n",
    "    ############################\n",
    "    \"hidden_size\": 128,\n",
    "    \"num_epochs\": 8,\n",
    "    \"batch_size\": 64,\n",
    "    #############################\n",
    "    ##### OPTIM & Scheduler #####\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"lr\": 3e-4,\n",
    "    \"betas\": (0.9, 0.999),\n",
    "    \"eps\": 1e-7,\n",
    "    \"weight_decay\": 1e-2,\n",
    "    \"lr_warm\": 1000,\n",
    "    \"lr_constant\": 2000,\n",
    "    # SMOTE\n",
    "    # label smooth\n",
    "    # CE weight / Focal loss alpha\n",
    "    # hidden size / expansion factor\n",
    "    ##############################\n",
    "    ######## FREQUENCIES #########\n",
    "    \"validation_freq\": 10,\n",
    "    \"checkpoint_freq\": 100,\n",
    "    ####### NON IMPORTANT ########\n",
    "    \"seed\": 42,\n",
    "    \"reproducible\": False,\n",
    "    \"experiment_name\": None,\n",
    "}\n",
    "\n",
    "# === AUTO SEEDING ===\n",
    "random.seed(CONFIG[\"seed\"])\n",
    "np.random.seed(CONFIG[\"seed\"])\n",
    "torch.manual_seed(CONFIG[\"seed\"])\n",
    "torch.cuda.manual_seed_all(CONFIG[\"seed\"])\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "if CONFIG[\"reproducible\"]:\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# === AUTO DEVICE ===\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc190bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashml import (\n",
    "    BatchIterator,\n",
    "    load_checkpoint,\n",
    "    log_checkpoint,\n",
    "    log_metrics,\n",
    ")\n",
    "\n",
    "### [Load data] TODO\n",
    "train_data = None\n",
    "eval_data = None\n",
    "test_data = None\n",
    "\n",
    "batch_iterator = BatchIterator(\n",
    "    df=train_data,\n",
    "    num_epochs=CONFIG[\"num_epochs\"],\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    mode=\"train\",\n",
    ")\n",
    "\n",
    "print(\"Data Loaded.\")\n",
    "\n",
    "#### [Define model] TODO\n",
    "model = None # AutoModelForCausalLM.from_pretrained(CONFIG[\"model_name\"], quantization_config=CONFIG[\"quantization_config\"], device_map=device)\n",
    "tokenizer = None # AutoTokenizer.from_pretrained(CONFIG[\"tokenizer_name\"])\n",
    "model.train()\n",
    "\n",
    "# LoRA training\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = False\n",
    "# for module in model.modules():\n",
    "#     if \"norm\" in module.__class__.__name__.lower():\n",
    "#         module.to(torch.float32)\n",
    "#         print(f\"{module.__class__.__name__.lower()} moved to float32.\")\n",
    "# or\n",
    "# model = peft.prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, CONFIG[\"lora_config\"]).cuda()\n",
    "print(\"Model Loaded.\")\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optim = AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=CONFIG[\"lr\"],\n",
    "    betas=CONFIG[\"betas\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    "    fused=\"True\",\n",
    ")\n",
    "\n",
    "from flashml.schedulers import LRScheduler\n",
    "scheduler = LRScheduler(\n",
    "    optimizer=optim,\n",
    "    max_steps=len(batch_iterator),\n",
    "    warmup_steps=CONFIG[\"lr_warm\"],\n",
    "    constant_steps=CONFIG[\"lr_constant\"],\n",
    "    annealing_type='cosine'\n",
    ")\n",
    "\n",
    "# === AUTO RESUMING ===\n",
    "if CONFIG[\"resume_training\"] and CONFIG[\"resume_training\"][\"run_id\"]:\n",
    "    ckpt = load_checkpoint(\n",
    "        run_id=CONFIG[\"resume_training\"][\"run_id\"],\n",
    "        version=CONFIG[\"resume_training\"][\"version\"],\n",
    "        experiment_name=CONFIG[\"experiment_name\"],\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    optim.load_state_dict(ckpt[\"optim_state_dict\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler_state_dict\"])\n",
    "    batch_iterator.load_state_dict(ckpt[\"batch_iterator_state_dict\"])\n",
    "    #### [OPTIONAL] TODO append other state dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a0ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, batch in batch_iterator:\n",
    "    ### ==================================================== Train ====================================================================================\n",
    "    model.train()\n",
    "\n",
    "    ### [] TODO forward backward\n",
    "    loss = None\n",
    "\n",
    "    if (step[0] + 1) % CONFIG[\"gradient_accumulation_steps\"] == 0:\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    scheduler.step()\n",
    "\n",
    "    log_metrics(\n",
    "        {\"loss\": loss.item(), \"lr\": scheduler.current_lr},\n",
    "        step=step,\n",
    "        hyperparams=CONFIG,\n",
    "        experiment_name=CONFIG[\"experiment_name\"],\n",
    "    )\n",
    "\n",
    "    ### =================================================== Validation ==============================================================================\n",
    "    if CONFIG[\"validation_freq\"] and CONFIG[\"validation_freq\"] > 0 and (\n",
    "        (step[0] > 0 and step[0] % CONFIG[\"validation_freq\"] == 0) or step[0] == step[1]\n",
    "    ):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            targs = []\n",
    "\n",
    "            for step_ev, batch_ev in BatchIterator(\n",
    "                df=eval_data,\n",
    "                num_epochs=1,\n",
    "                batch_size=CONFIG[\"batch_size\"] * 2,\n",
    "                mode=\"eval\",\n",
    "            ):\n",
    "                pass  ### [] TODO eval\n",
    "\n",
    "            eval_metrics = None  # compute_metrics(preds, targs)\n",
    "\n",
    "            log_metrics(\n",
    "                metrics=eval_metrics,\n",
    "                step=step,\n",
    "                experiment_name=CONFIG[\"experiment_name\"],\n",
    "            )\n",
    "    ### =================================================== Checkpointing ==============================================================================\n",
    "    if CONFIG[\"checkpoint_freq\"] and CONFIG[\"checkpoint_freq\"] > 0 and (\n",
    "        (step[0] > 0 and step[0] % CONFIG[\"checkpoint_freq\"] == 0) or step[0] == step[1]\n",
    "    ):\n",
    "        print(f\"Checkpoint made: {step[0]}/{step[1]}\")\n",
    "        log_checkpoint(\n",
    "            state_dict={\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optim_state_dict\": optim.state_dict(),\n",
    "                \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                \"batch_iterator_state_dict\": batch_iterator.state_dict(),\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f29bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HOST MLFLOW (without logging anything) ###\n",
    "from flashml import host_mlflow\n",
    "\n",
    "host_mlflow()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
